Greedy Algorithm: The greedy algorithm is a simple approach that selects the items with the highest value-to-weight ratio and adds them to the knapsack until the knapsack is full or there are no more items left. This algorithm is fast and easy to implement, but may not always produce the optimal solution.

Dynamic Programming: Dynamic programming is a more sophisticated approach that breaks down the problem into smaller subproblems and solves them iteratively. The solution to the original problem is then built up from the solutions to the subproblems. This algorithm guarantees an optimal solution, but may be computationally expensive for large instances of the problem.

Branch and Bound: Branch and bound is a search algorithm that explores the solution space by dividing it into smaller subproblems and bounding the solution space for each subproblem. The algorithm iteratively explores the subproblems with the highest potential for producing an optimal solution and prunes those that are unlikely to do so. This algorithm also guarantees an optimal solution, but may be computationally expensive for large instances of the problem.

Genetic Algorithms: Genetic algorithms are a type of evolutionary algorithm that use a population of candidate solutions and iteratively evolve them by applying genetic operators such as mutation and crossover. The fitness of each solution is evaluated based on its value and weight, and the fittest solutions are selected for further evolution. This algorithm is often used when the problem is too complex for traditional algorithms or when the goal is to find a good solution quickly rather than an optimal one.

These are just a few examples of algorithms that can be applied to the knapsack problem. Other algorithms such as linear programming, simulated annealing, and ant colony optimization have also been used with varying degrees of success. The choice of algorithm depends on factors such as the size of the problem, the desired level of optimality, and the available computing resources.


Hill climbing is a class of local search algorithms that starts with an initial solution and iteratively moves to a better solution by making small modifications to the current solution. There are several variants of hill climbing algorithms, including:

Basic hill climbing: This algorithm starts with an initial solution and iteratively moves to a better solution by making small modifications to the current solution. The algorithm stops when no further improvements can be made to the current solution.

Steepest ascent hill climbing: This algorithm evaluates all possible neighboring solutions and selects the one that provides the steepest improvement in the objective function. This ensures that the algorithm is always moving in the direction of the best solution.

First-choice hill climbing: This algorithm evaluates a random set of neighboring solutions and selects the first one that provides an improvement in the objective function. This allows the algorithm to escape from local optima.

Random-restart hill climbing: This algorithm performs multiple hill climbing searches with different initial solutions and selects the best solution among them. This allows the algorithm to explore different areas of the search space and avoid getting stuck in local optima.

Simulated annealing: This algorithm starts with a high temperature and gradually decreases it over time. At high temperatures, the algorithm is more likely to accept worse solutions, allowing it to escape from local optima. As the temperature decreases, the algorithm becomes more selective in accepting worse solutions.



For this assignment i am going to be using the "Next Steepest Ascent Hill Climbing" algorithm and logging the results.

I also implemented a logging method for each print out in the console.

Tested for 30n knapsack and 100n knapsack. On 100, 1000 and 100k runs.